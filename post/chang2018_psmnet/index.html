<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Pyramid Stereo Matching Network (PSMNet) - memo</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="nohzen" /><meta name="description" content="J.-R. Chang, et al. &amp;ldquo;Pyramid Stereo Matching Network&amp;rdquo; (2018) CVPR 2018 概要 Pyramid Stereo Matching Network (PSMNet)と名付けた視差推定のネットワークを提案 [GCNet]をベースとする 画像のグローバルなコンテ" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.75.1 with theme even" />


<link rel="canonical" href="https://nohzen.github.io/Hugo_blog/post/chang2018_psmnet/" />
<link rel="apple-touch-icon" sizes="180x180" href="https://nohzen.github.io/Hugo_blog/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://nohzen.github.io/Hugo_blog/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://nohzen.github.io/Hugo_blog/favicon-16x16.png">
<link rel="manifest" href="https://nohzen.github.io/Hugo_blog/manifest.json">
<link rel="mask-icon" href="https://nohzen.github.io/Hugo_blog/safari-pinned-tab.svg" color="#5bbad5">



<link href="https://nohzen.github.io/Hugo_blog/sass/main.min.c7bc1becf36bcf6a9ebd25d2947e43a2eb745ddb0c9a32b43126fd7fa460c351.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Pyramid Stereo Matching Network (PSMNet)" />
<meta property="og:description" content="J.-R. Chang, et al. &ldquo;Pyramid Stereo Matching Network&rdquo; (2018) CVPR 2018 概要 Pyramid Stereo Matching Network (PSMNet)と名付けた視差推定のネットワークを提案 [GCNet]をベースとする 画像のグローバルなコンテ" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://nohzen.github.io/Hugo_blog/post/chang2018_psmnet/" />
<meta property="article:published_time" content="2020-12-06T05:33:22+09:00" />
<meta property="article:modified_time" content="2020-12-06T05:33:22+09:00" />
<meta itemprop="name" content="Pyramid Stereo Matching Network (PSMNet)">
<meta itemprop="description" content="J.-R. Chang, et al. &ldquo;Pyramid Stereo Matching Network&rdquo; (2018) CVPR 2018 概要 Pyramid Stereo Matching Network (PSMNet)と名付けた視差推定のネットワークを提案 [GCNet]をベースとする 画像のグローバルなコンテ">
<meta itemprop="datePublished" content="2020-12-06T05:33:22+09:00" />
<meta itemprop="dateModified" content="2020-12-06T05:33:22+09:00" />
<meta itemprop="wordCount" content="2217">



<meta itemprop="keywords" content="Stereo Matching," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Pyramid Stereo Matching Network (PSMNet)"/>
<meta name="twitter:description" content="J.-R. Chang, et al. &ldquo;Pyramid Stereo Matching Network&rdquo; (2018) CVPR 2018 概要 Pyramid Stereo Matching Network (PSMNet)と名付けた視差推定のネットワークを提案 [GCNet]をベースとする 画像のグローバルなコンテ"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="https://nohzen.github.io/Hugo_blog/" class="logo">memo</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="https://nohzen.github.io/Hugo_blog/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="https://nohzen.github.io/Hugo_blog/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="https://nohzen.github.io/Hugo_blog/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="https://nohzen.github.io/Hugo_blog/" class="logo">memo</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="https://nohzen.github.io/Hugo_blog/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://nohzen.github.io/Hugo_blog/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://nohzen.github.io/Hugo_blog/tags/">Tags</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Pyramid Stereo Matching Network (PSMNet)</h1>

      <div class="post-meta">
        <span class="post-time"> 2020-12-06 </span>
        
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">コンテンツ</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#概要">概要</a></li>
    <li><a href="#論文情報">論文情報</a>
      <ul>
        <li><a href="#リンク">リンク</a></li>
        <li><a href="#著者">著者</a></li>
      </ul>
    </li>
    <li><a href="#内容">内容</a>
      <ul>
        <li><a href="#手法">手法</a>
          <ul>
            <li><a href="#spatial-pyramid-pooling-spp-module">Spatial Pyramid Pooling (SPP) module</a></li>
            <li><a href="#cost-volume">Cost Volume</a></li>
            <li><a href="#stacked-multiple-hourglass-3d-conv">Stacked multiple hourglass 3D Conv</a></li>
            <li><a href="#視差出力">視差出力</a></li>
            <li><a href="#回帰-loss">回帰 Loss</a></li>
          </ul>
        </li>
        <li><a href="#実験">実験</a></li>
      </ul>
    </li>
    <li><a href="#感想">感想</a></li>
    <li><a href="#参考文献">参考文献</a></li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p><a href="https://arxiv.org/abs/1803.08669">J.-R. Chang, et al. &ldquo;Pyramid Stereo Matching Network&rdquo; (2018) CVPR 2018</a></p>
<h1 id="概要">概要</h1>
<ul>
<li><strong>Pyramid Stereo Matching Network</strong> (PSMNet)と名付けた視差推定のネットワークを提案
<ul>
<li>[GCNet]をベースとする</li>
<li>画像のグローバルなコンテキストを活用できる<strong>Spatial Pyramid Pooling Module</strong>を特徴量抽出に利用</li>
<li>cost volumeの正則化は、<strong>Hourglass 構造(Encoder-Decoder)の3D convを複数回</strong>使い、<strong>中間出力からも視差を学習</strong>する</li>
</ul>
</li>
</ul>
<h1 id="論文情報">論文情報</h1>
<h2 id="リンク">リンク</h2>
<ul>
<li><a href="https://github.com/JiaRenChang/PSMNet">公式実装 PyTorch</a></li>
</ul>
<h2 id="著者">著者</h2>
<p>台湾のNational Chiao Tung University (NCTU, 国立交通大学)のメンバー</p>
<ul>
<li><a href="https://jiarenchang.github.io/">Jia-Ren Chang</a></li>
<li><a href="https://people.cs.nctu.edu.tw/~yschen/">Yong-Sheng Chen</a></li>
</ul>
<h1 id="内容">内容</h1>
<h2 id="手法">手法</h2>
<figure class="center">
    <img src="https://nohzen.github.io/Hugo_blog/images/chang2018_PSMNet/Fig1.png"
         alt="[Chang&#43;(2018)] Fig. 1より引用" width="1000"/> <figcaption>
            <p>[Chang+(2018)] Fig. 1より引用</p>
        </figcaption>
</figure>

<ul>
<li><strong>GCNetをベース</strong>としている。違いは以下の3点:
<ul>
<li>特徴量抽出に<strong>Spatial Pyramid Pooling</strong> (SPP) moduleを追加することで、グローバルなコンテキストを利用</li>
<li>Cost volumeの正規化を行う3D convに<strong>複数回のHourglass構造</strong>を使い、<strong>中間出力からも学習</strong>する(GCNetは1回のHourglass構造)</li>
<li><strong>smooth L1 loss</strong>で回帰問題として扱う(GCNetはL1 loss)</li>
</ul>
</li>
</ul>
<h3 id="spatial-pyramid-pooling-spp-module">Spatial Pyramid Pooling (SPP) module</h3>
<ul>
<li>Textureがない領域や繰り返しパターンの領域では、広い範囲の情報(グローバルなコンテキスト)を利用する必要がある</li>
<li>そのために、<strong>Spatial pyramid pooling</strong> (SPP)と<strong>Dilated conv</strong>を利用してreceptive fieldを広げた特徴量を作り、それを元にcost volumeを作成する
<ul>
<li>畳み込み層は、実効的なreceptive filedが理論的な最大範囲ほど広くならないので、Spatial pyramid poolingを利用</li>
</ul>
</li>
<li>ほとんど<strong>PSPNet</strong>のPyramid Pooling Moduleと同じ(Ave. poolingのサイズだけが違う)
<ul>
<li>入力特徴量サイズH/4xW/4x128に依らず、出力が64x64, 32x32, 16x16, 8x8のAverage pooling(PSPNetは、1x1, 2x2, 3x3, 6x6のAverage pooling)</li>
<li>各Average poolingの出力に対して、convで特徴量次元を128から32に減らす(本文では1x1 convになっていて、Table 1では3x3 convになっている？PSPNetは1x1 conv)</li>
<li>元解像度(H/4xW/4)にbilinear upscale</li>
<li>入力特徴量と4つのスケールのPooling結果をconcate(conv2の特徴量もconcateする)</li>
</ul>
</li>
</ul>
<h3 id="cost-volume">Cost Volume</h3>
<ul>
<li>GCNetと同様に、左右の画像のSPP moduleの出力特徴量を<strong>concate</strong>することで、4D cost volumeを作成</li>
</ul>
<h3 id="stacked-multiple-hourglass-3d-conv">Stacked multiple hourglass 3D Conv</h3>
<p>cost volumeの正則化として、<strong>basic architecture</strong>と<strong>stacked hourglass architecture</strong>の2種類を提案</p>
<ul>
<li>basic architecture: 12個の3x3x3 conv Residual block -&gt; bilinear upsampleで元解像度に</li>
<li>stacked hourglass(encorder-decoder) architecture: <strong>3個のhourglass構造</strong>で、各hourglassの後でbilinear upsampleした後に視差を計算する。学習時は<strong>中間出力からの視差も学習</strong>し、テスト時は最終出力からの視差のみ使う。GCNetは1個のhourglassに相当(層の数などは違う)。</li>
</ul>
<h3 id="視差出力">視差出力</h3>
<ul>
<li>GCNetと同様に、<strong>コストのSoftmaxを確率として加重平均</strong>したものを視差の推論結果とする
<ul>
<li>これにより微分可能（学習可能）で、分類問題として扱うよりロバストになる</li>
</ul>
</li>
</ul>
<h3 id="回帰-loss">回帰 Loss</h3>
<ul>
<li>物体検出のBounding Boxの回帰に使われる<strong>smooth-L1 loss</strong> (小さいところではL2で、大きいところではL1)を使う
<ul>
<li>L2より外れ値に強くなる</li>
<li>GCNetはL1 lossを使っている</li>
</ul>
</li>
</ul>
<h2 id="実験">実験</h2>
<ul>
<li>Scene Flow: 視差が大きすぎるPixelも含まれるので、一定以上に視差の場合はLossをゼロにする。事前学習なし。</li>
<li>KITTI2012, 2015: Scene Flowで事前学習</li>
<li>学習時は、画像をランダムに512x256にCrop(spatial pyramid poolingが含まれるので、学習時と推論時でサイズが違うと良くないのでは？)</li>
<li>ablation study (KITTI2015で)
<ul>
<li>dilation convありの方がいい</li>
<li>pyramid poolinありの方がいい</li>
<li>basic architectureよりstacked hourglass architectureの方がいい</li>
<li>中間出力のLossの重みは、最終出力に近い方を大きめにする方がいい</li>
</ul>
</li>
</ul>
<h1 id="感想">感想</h1>
<ul>
<li>手法自体は、GCNetから順当な小さな改善をした感じ。コードをしっかり公開しているので、引用数が伸びてそう。</li>
<li>3D convのところは、basic architectureは中間出力なしで、stacked hourglass architectureだけが中間出力ありなのは理由がある？フェアに比較するなら、揃えた方が良さそう。
<ul>
<li>stacked hourglass architectureは学習しにくいから、中間出力からも学習している？</li>
<li>Table2, 3を見ると、中間出力なしのbasic architectureの方が、中間出力なしのstacked hourglass architectureより精度がいい。中間出力ありのbasic architectureは試していないのが気になる。中間出力ありで比較すると、basicの方がstacked hourglassより良くなったりしない？</li>
</ul>
</li>
<li>Scene Flowも特徴量抽出のところはImageNetで事前学習しておくと、精度が上がったりしない？</li>
<li>実験結果で、”by a noteworthy margin”で先行研究よりいいと書かれているが、そこまで大きく改善している訳ではない。Deep learningの論文にありがちなんだけど、無駄な形容詞で結果を良く言うのが嫌い。</li>
</ul>
<h1 id="参考文献">参考文献</h1>
<ul>
<li>Stereo Matching
<ul>
<li>[13] [GCNet] A. Kendall, et al. &ldquo;End-to-end learning of geometry and context for deep stereo regression&rdquo; (2017)</li>
</ul>
</li>
<li>Spatial pyramid pooling (SPP)
<ul>
<li>[9] [SPP-net] K. He, et al. &ldquo;Spatial pyramid pooling in deep convolutional networks for visual recognition&rdquo; (2014)</li>
<li>[16] [ParseNet] W. Liu, et al. &ldquo;ParseNet: Looking wider to see better&rdquo; (2015)</li>
<li>[32] [PSPNet] H. Zhao, et al. &ldquo;Pyramid scene parsing network&rdquo; (2017)</li>
</ul>
</li>
<li>Dilated conv
<ul>
<li>[2] L.-C. Chen, et al. &ldquo;DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs&rdquo; (2016)</li>
<li>[29] F. Yu et al. &ldquo;Multi-scale context aggregation by dilated convolutions&rdquo; (2016)</li>
</ul>
</li>
</ul>

    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="https://nohzen.github.io/Hugo_blog/tags/stereo-matching/">Stereo Matching</a>
          </div>
      <nav class="post-nav">
        
        <a class="next" href="https://nohzen.github.io/Hugo_blog/post/zhao2016_pspnet/">
            <span class="next-text nav-default">Pyramid Scene Parsing Network (PSPNet)</span>
            <span class="next-text nav-mobile">次の記事へ</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="https://nohzen.github.io/Hugo_blog/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2020<span class="heart"><i class="iconfont icon-heart"></i></span><span>nohzen</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="https://nohzen.github.io/Hugo_blog/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>








</body>
</html>
